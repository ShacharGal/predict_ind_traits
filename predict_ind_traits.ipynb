{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.model_selection import LeaveOneOut, KFold, LeaveOneOut, train_test_split\n",
    "from sklearn.linear_model import LinearRegression, ElasticNetCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "import scipy as sp\n",
    "from scipy.stats import pearsonr\n",
    "pd.options.mode.chained_assignment = None\n",
    "from IPython.core.debugger import set_trace\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import nibabel as nib\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some functions used later in the main functions\n",
    "# or are helpers for reading and handling data\n",
    "\n",
    "def match_dfs_by_ind(df_list, behav, to_compare=[]):\n",
    "    # if we dont have the same subjects available for all tasks, it matches it\n",
    "    all_indices = [list(df.index) for df in df_list]\n",
    "    all_indices.append(behav.index)\n",
    "    if len(to_compare):\n",
    "        all_indices.append(to_compare.index)\n",
    "    in_all = list(set(all_indices[0]).intersection(*all_indices))\n",
    "    return [df[df.index.isin(in_all)] for df in df_list], behav[behav.index.isin(in_all)]\n",
    "    \n",
    "def read_features(to_use, subjlist='test_subjlist', to_scale=True):\n",
    "    orig_mat = np.genfromtxt(f'data/{to_use}.csv', delimiter=',')\n",
    "    subjects_path = f'data/{subjlist}.txt'\n",
    "    with open(subjects_path, 'r') as f:\n",
    "        subjects = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "    # arrange features in df and keep only subjects that we have g scores for\n",
    "    features = pd.DataFrame(data=orig_mat, index=subjects)\n",
    "    \n",
    "    if to_scale:\n",
    "        # scale features\n",
    "        scaler = StandardScaler()\n",
    "        scaled_features = pd.DataFrame(data = scaler.fit_transform(features), index = features.index)\n",
    "        \n",
    "        return scaled_features\n",
    "    else:\n",
    "        return features\n",
    "\n",
    "def decompose(data, num_comps, transformer='pca'):\n",
    "    # recieves a subjectsXvertices matrix. returns verticesXcomponents matrix\n",
    "    if transformer == 'pca':\n",
    "        trans = PCA()\n",
    "    if transformer == 'ica':\n",
    "        trans = FastICA(max_iter=500)\n",
    "    trans.fit(data)\n",
    "    return trans.components_.T[:,:num_comps]\n",
    "\n",
    "def demean_by_train(train, test):\n",
    "    # demeans the data by the training sets mean\n",
    "\n",
    "    train_avg = train.mean(axis=0)\n",
    "    train_demeaned = train-train_avg\n",
    "    test_demeaned = test-train_avg\n",
    "    return train_demeaned, test_demeaned\n",
    "\n",
    "def shuffle_copy(to_shuffle):\n",
    "    # returns shuffled DataFrame to utilize in permutation tests\n",
    "    \n",
    "    shuffled = to_shuffle.copy()\n",
    "    shuffled = shuffled.values\n",
    "    np.random.shuffle(shuffled)\n",
    "    return shuffled\n",
    "\n",
    "\n",
    "def corr_analysis(features, y, ff_num):\n",
    "    # select featrues cpm-style\n",
    "    feat_num = features.shape[1]\n",
    "    corrs = np.zeros(feat_num)\n",
    "    for feat in range(feat_num):\n",
    "        corrs[feat] = sp.stats.pearsonr(features[:,feat],y)[0]\n",
    "    mask = abs(corrs)>=np.sort(abs(corrs))[len(corrs)-ff_num]\n",
    "    return mask\n",
    "\n",
    "def save_masked_maps(data, filename):\n",
    "    template = nib.load('data/Smask.dtseries.nii')\n",
    "    mask = np.asanyarray(template.dataobj)\n",
    "    mask[mask==1]=data\n",
    "    to_save = nib.cifti2.cifti2.Cifti2Image(mask, template.header)\n",
    "    nib.save(to_save, f'{filename}.dtseries.nii')\n",
    "    \n",
    "def get_task_feat_num(mask, num_comps):\n",
    "    num_tasks = len(mask)/num_comps\n",
    "    f_per_task = []\n",
    "    for task in range(int(num_tasks)):\n",
    "        f_per_task.append((mask[num_comps*task: num_comps*(task+1)].sum()))\n",
    "    return f_per_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# two functions - one for single data entry and one for multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_bbs_single(features, score_df, reg_type, k=10, num_comps=75, score='g_efa', save_maps=False):\n",
    "    \n",
    "    stats = {'r':[], 'mse':[]}\n",
    "    \n",
    "    kfold = KFold(n_splits=k, random_state=42, shuffle=True)\n",
    "    predicted = np.zeros(features.shape[0])\n",
    "    \n",
    "    if permute_num:\n",
    "        permutations = np.zeros((features.shape[0],permute_num-1))\n",
    "        \n",
    "    consensus = np.zeros((features.shape[1]))\n",
    "    \n",
    "    for fold, (train_index, test_index) in enumerate(kfold.split(features)):\n",
    "        print(f'fold {fold+1} out of {k}')\n",
    "        train_features = np.zeros([len(train_index),num_comps*len(dfs_list)])\n",
    "        test_features = np.zeros([len(test_index),num_comps*len(dfs_list)])\n",
    "        \n",
    "        fold_comps = np.zeros((features.shape[1], num_comps))\n",
    "        for i in range(len(dfs_list)):\n",
    "            features = dfs_list[i]\n",
    "            X_train, X_test = features.iloc[train_index,:], features.iloc[test_index,:]\n",
    "\n",
    "            Y_train, Y_test = score_df[score].iloc[train_index], score_df[score].iloc[test_index]\n",
    "            # create pca-reduced matrix, in the shpae of verticesXcomponents\n",
    "            reduced_train = decompose(X_train, num_comps, transformer='pca')\n",
    "\n",
    "            # extract individual features by calculating expression scores for each subject\n",
    "            this_train_features = np.matmul(np.linalg.pinv(reduced_train),X_train.values.T).T            \n",
    "            this_test_features = np.matmul(np.linalg.pinv(reduced_train),X_test.values.T).T\n",
    "            \n",
    "            start = i*num_comps\n",
    "            end = start+num_comps\n",
    "            train_features[:, start:end] = this_train_features\n",
    "            test_features[:, start:end] = this_test_features\n",
    "            \n",
    "            # save the components used for feature extration \n",
    "            if save_maps: \n",
    "                fold_comps[:,start:end] = reduced_train\n",
    "                \n",
    "        \n",
    "        # calculate model and get predictions\n",
    "        if reg_type == 'glm':\n",
    "            model = LinearRegression()\n",
    "        elif reg_type == 'elnet':\n",
    "            model = ElasticNetCV(l1_ratio=[0.9, 0.95, 0.99],n_alphas=50)\n",
    "        \n",
    "        model.fit(train_features, Y_train)            \n",
    "        predicted[test_index] = model.predict(test_features)\n",
    "        \n",
    "        if return_stats:\n",
    "            stats['r'].append(sp.stats.pearsonr(predicted[test_index], Y_test)[0])\n",
    "            stats['mse'].append(mean_squared_error(predicted[test_index], Y_test))\n",
    "        \n",
    "        if save_maps:\n",
    "            # weight components with their related beta values\n",
    "            weighted_task_comps = fold_comps*model.coef_\n",
    "            # sum over components to get a single weighted map for this fold\n",
    "            summed_weighted_task_comps = np.sum(weighted_task_comps,axis=1)\n",
    "            # add the weighted components to the rest of the weighted components.\n",
    "            consensus += summed_weighted_task_comps\n",
    "        \n",
    "    if return_stats:\n",
    "        summary = pd.DataFrame(stats).describe().loc[['mean', 'std'],:]\n",
    "        if save_maps:\n",
    "            return predicted, stats, summary, consensus\n",
    "        else:\n",
    "            return predicted, stats, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_bbs_multi_data_select(dfs_list, task_names, score_df, reg_type, k=10, num_comps=75, score='g_efa', permute_num=False, ff_num = 75, l1_ratio=0.01, save_maps=False):\n",
    "    \n",
    "    stats = {'r':[], 'mse':[]}\n",
    "\n",
    "    kfold = KFold(n_splits=k, random_state=42, shuffle=True)\n",
    "    feature_example = dfs_list[0]\n",
    "    predicted = np.zeros(feature_example.shape[0])\n",
    "    \n",
    "    task_comps_per_fold = []                     \n",
    "    consensus = np.zeros((feature_example.shape[1]))\n",
    "    \n",
    "    for fold, (train_index, test_index) in enumerate(kfold.split(feature_example)):\n",
    "        print(f'fold {fold+1} out of {k}')\n",
    "        train_features = np.zeros([len(train_index),num_comps*len(dfs_list)])\n",
    "        test_features = np.zeros([len(test_index),num_comps*len(dfs_list)])\n",
    "        # define this folds' Y\n",
    "        Y_train, Y_test = score_df[score].iloc[train_index], score_df[score].iloc[test_index]\n",
    "\n",
    "        fold_comps = np.zeros((features.shape[1], num_comps*len(dfs_list)))\n",
    "\n",
    "        for i in range(len(dfs_list)):\n",
    "            features = dfs_list[i]\n",
    "            X_train, X_test = features.iloc[train_index,:], features.iloc[test_index,:]\n",
    "\n",
    "            # create pca-reduced matrix, in the shpae of verticesXcomponents\n",
    "            reduced_train = decompose(X_train, num_comps, transformer='pca')\n",
    "\n",
    "            # extract individual features by calculating expression scores for each subject\n",
    "            this_train_features = np.matmul(np.linalg.pinv(reduced_train),X_train.values.T).T            \n",
    "            this_test_features = np.matmul(np.linalg.pinv(reduced_train),X_test.values.T).T\n",
    "            \n",
    "            start = i*num_comps\n",
    "            end = start+num_comps\n",
    "            train_features[:, start:end] = this_train_features\n",
    "            test_features[:, start:end] = this_test_features\n",
    "            \n",
    "            # save the components used for feature extration \n",
    "            if save_maps: \n",
    "                fold_comps[:,start:end] = reduced_train\n",
    "        \n",
    "\n",
    "        # correlation analysis to select features\n",
    "        mask = corr_analysis(train_features, Y_train, ff_num)\n",
    "        task_comps_per_fold.append(get_task_feat_num(mask, num_comps))\n",
    "        \n",
    "        # reduce features with mask produced in the correlation analysis\n",
    "        train_features = train_features[:, mask]\n",
    "        test_features = test_features[:, mask]\n",
    "        \n",
    "        # calculate model and get predictions\n",
    "        if reg_type == 'glm':\n",
    "            model = LinearRegression()\n",
    "        elif reg_type == 'elnet':\n",
    "            model = ElasticNetCV(l1_ratio=l1_ratio,n_alphas=50, tol=0.001, max_iter=5000)\n",
    "            \n",
    "        model.fit(train_features, Y_train)\n",
    "        betas = model.coef_\n",
    "                    \n",
    "        predicted[test_index] = model.predict(test_features)\n",
    "        \n",
    "        stats['r'].append(sp.stats.pearsonr(predicted[test_index], Y_test)[0])\n",
    "        stats['mse'].append(mean_squared_error(predicted[test_index], Y_test))\n",
    "\n",
    "        if save_maps:\n",
    "            #reduce fold_comps according to the correlation analysis\n",
    "            masked_comps = fold_comps[:,mask]\n",
    "            # weight components with their related beta values\n",
    "            weighted_task_comps = masked_comps*betas\n",
    "            # sum over components to get a single weighted map for this fold\n",
    "            summed_weighted_task_comps = np.sum(weighted_task_comps,axis=1)\n",
    "            # add the weighted components to the rest of the weighted components.\n",
    "            consensus += summed_weighted_task_comps\n",
    "        \n",
    "    summary = pd.DataFrame(stats).describe().loc[['mean', 'std'],:]\n",
    "        \n",
    "    task_comps_per_fold_df = pd.DataFrame(data = task_comps_per_fold, columns = task_names)\n",
    "    if save_maps:\n",
    "        return predicted, stats, summary, task_comps_per_fold_df, consensus\n",
    "    else:\n",
    "        return predicted, stats, summary, task_comps_per_fold_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check significance of comparison between inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_predictions(A,B,hcp_df,score,k=30, num_comps=75):\n",
    "    dfs, behav = match_dfs_by_ind([A,B], hcp_df, to_compare=emot_facshp_orig_z)\n",
    "    \n",
    "    score_dict = {0:[], 1:[]}\n",
    "    for i in range(k):\n",
    "        print(i)\n",
    "        rand_seed = np.random.randint(0,1000)\n",
    "        for m in [0,1]:\n",
    "            \n",
    "            X_train, X_test, y_train, y_test = train_test_split(dfs[m], behav[score], test_size=0.33, random_state=rand_seed)\n",
    "            reduced_train = decompose(X_train, num_comps, transformer='pca')\n",
    "\n",
    "            # extract individual features by calculating expression scores for each subject\n",
    "            train_features = np.matmul(np.linalg.pinv(reduced_train),X_train.values.T).T            \n",
    "            test_features = np.matmul(np.linalg.pinv(reduced_train),X_test.values.T).T\n",
    "            model = LinearRegression()\n",
    "            model.fit(train_features, y_train)\n",
    "            predicted = model.predict(test_features)\n",
    "            r = pearsonr(predicted,y_test)[0]\n",
    "            score_dict[m].append(r)\n",
    "            \n",
    "    m,p = sp.stats.mannwhitneyu(score_dict[0], score_dict[1]) \n",
    "    return p\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get data of mean activity in DMN and FPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrasts = {'2bk>0bk' : 'WM_11_s4', '2bk': 'WM_09_s4', '0bk': 'WM_10_s4',\n",
    "        'Math-Story' : 'Lang_03_s4',\n",
    "        'Random' : 'Soc_01_s4', 'TOM': 'Soc_02_s4', 'TOM-Radnom': 'Soc_06_s4',\n",
    "        'Rel' : 'Rel_02_s4', 'Match': 'Rel_01_s4', 'Rel-Match': 'Rel_04_s4',\n",
    "        'Reweard': 'Gamb_02_s4', 'Punish': 'Gamb_01_s4', 'Punish-Reward': 'Gamb_03_s4',\n",
    "        'Faces-Shapes': 'Em_03_s4'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yeo_parc = nib.load('/Volumes/HCP/HCP_WB_Tutorial_1.0/yeo_masked.dtseries.nii')\n",
    "yeo_parc = np.asanyarray(yeo_parc.dataobj)\n",
    "fpn_num = 6;\n",
    "dmn_num = 7;\n",
    "fpn_mask = (yeo_parc==fpn_num).flatten()\n",
    "dmn_mask = (yeo_parc==dmn_num).flatten()\n",
    "\n",
    "data_dir = '/Volumes/HCP/Predicted_data_100';\n",
    "\n",
    "orig_fpn_mean=[]\n",
    "orig_dmn_mean=[]\n",
    "pred_fpn_mean=[]\n",
    "pred_dmn_mean=[]\n",
    "contrast_description = list(contrasts.keys())\n",
    "contrast_names = list(contrasts.values())\n",
    "\n",
    "for con in contrast_names:\n",
    "    print(con)\n",
    "    all_orig_path = f'{data_dir}/{con}/all_test_data/all_test_data_orig_z.dtseries.nii'\n",
    "    all_orig = nib.load(all_orig_path)\n",
    "    all_orig = np.asanyarray(all_orig.dataobj)\n",
    "    \n",
    "    orig_fpn_mean.append(np.mean(np.mean(all_orig[:, fpn_mask])));\n",
    "    orig_dmn_mean.append(np.mean(np.mean(all_orig[:, dmn_mask])));\n",
    "    \n",
    "    all_pred_path = f'{data_dir}/{con}/all_test_data/all_test_data_pred_z_cleaned_cb.dtseries.nii'\n",
    "    all_pred = nib.load(all_pred_path)\n",
    "    all_pred = np.asanyarray(all_pred.dataobj)\n",
    "    \n",
    "    pred_fpn_mean.append(np.mean(np.mean(all_pred[:, fpn_mask])));\n",
    "    pred_dmn_mean.append(np.mean(np.mean(all_pred[:, dmn_mask])));\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
