{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneOut, KFold, LeaveOneOut, train_test_split, GroupShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression, ElasticNetCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy as sp\n",
    "from scipy.stats import pearsonr\n",
    "pd.options.mode.chained_assignment = None\n",
    "from IPython.core.debugger import set_trace\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import nibabel as nib\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from BBS import bbs, bbs_comparisons\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that helps to read in data into a dataframe with subjects as index\n",
    "def read_features(path, subjlist, to_scale=True):\n",
    "    orig_mat = np.genfromtxt(path, delimiter=',')\n",
    "    with open(subjlist, 'r') as f:\n",
    "        subjects = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "    # arrange features in df and keep only subjects that we have g scores for\n",
    "    features = pd.DataFrame(data=orig_mat, index=subjects)\n",
    "    \n",
    "    if to_scale:\n",
    "        # scale features\n",
    "        scaler = StandardScaler()\n",
    "        scaled_features = pd.DataFrame(data = scaler.fit_transform(features), index = features.index)\n",
    "        \n",
    "        return scaled_features\n",
    "    else:\n",
    "        return features\n",
    "    \n",
    "def save_masked_maps(data, filename):\n",
    "    template = nib.load('../BBS/misc/Smask.dtseries.nii')\n",
    "    mask = np.asanyarray(template.dataobj)\n",
    "    mask[mask==1]=data\n",
    "    to_save = nib.cifti2.cifti2.Cifti2Image(mask, template.header)\n",
    "    nib.save(to_save, f'{filename}.dtseries.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read hcp's behavioral data\n",
    "g_efa_cv = pd.read_csv('g_efa_cv.csv')\n",
    "g_efa_cv = g_efa_cv['x']\n",
    "\n",
    "hcp_df = pd.read_csv('hcp_dataframe_with_g.csv')\n",
    "hcp_df['Subject'] = hcp_df['Subject'].apply(str)\n",
    "hcp_df = hcp_df.set_index('Subject')\n",
    "hcp_df = hcp_df[['PMAT24_A_CR', 'ReadEng_Unadj', 'Gender', 'G', 'g_efa', 'g_cfa', 'NEOFAC_O', 'NEOFAC_C', 'NEOFAC_E', 'NEOFAC_A', 'NEOFAC_N']]\n",
    "hcp_df['g_efa_cv'] = g_efa_cv.values\n",
    "hcp_df = hcp_df.dropna()\n",
    "print(hcp_df.shape)\n",
    "\n",
    "# add family groups\n",
    "restricted_df = pd.read_csv('RESTRICTED_HCP.csv')\n",
    "restricted_df['Subject'] = restricted_df['Subject'].apply(str)\n",
    "restricted_df = restricted_df.set_index('Subject')\n",
    "restricted_df = restricted_df.loc[hcp_df.index.values,:]\n",
    "groups = [np.where(np.unique(restricted_df['Family_ID'])==family_id)[0][0] for family_id in restricted_df['Family_ID']]\n",
    "groups = np.array(groups, dtype=int)\n",
    "hcp_df['Family_group'] = groups\n",
    "\n",
    "hcp_df_wm = pd.read_csv('/Users/crazyjoe/Downloads/unrestricted_shachargal_7_7_2020_7_22_11.csv')\n",
    "hcp_df_wm['Subject'] = hcp_df_wm['Subject'].apply(str)\n",
    "hcp_df_wm = hcp_df_wm.set_index('Subject')\n",
    "hcp_df_wm = hcp_df_wm[['WM_Task_Acc', 'WM_Task_2bk_Acc']]\n",
    "hcp_df_wm = hcp_df_wm.dropna()\n",
    "hcp_df_wm.shape\n",
    "hcp_df_wm['Family_group'] = hcp_df['Family_group'][hcp_df_wm.index]\n",
    "hcp_df_wm = hcp_df_wm.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some important variables for later\n",
    "subjlist = '/Volumes/HCP/FE_100_noRelatives/all_subjects.txt'\n",
    "data_dir = '/Volumes/homes/Shachar/python_projects/bbs_prediction/data'\n",
    "task_contrasts = ['WM_09_s4','WM_10_s4','WM_11_s4',\n",
    "                  'Em_03_s4','Gamb_01_s4','Gamb_02_s4','Lang_03_s4',\n",
    "                'Rel_01_s4','Rel_02_s4','Rel_04_s4','Soc_01_s4','Soc_02_s4','Soc_06_s4']\n",
    "glm = LinearRegression()\n",
    "elnet = ElasticNetCV(l1_ratio=0.01, n_alphas=50, tol=0.001, max_iter=5000)\n",
    "combinations = [\n",
    "                ['WM_09_s4', 'Em_03_s4'], \n",
    "                ['WM_09_s4', 'Rel_02_s4'],\n",
    "                ['WM_09_s4', 'Lang_03_s4'],\n",
    "                ['WM_09_s4', 'Soc_01_s4'],\n",
    "                ['Lang_03_s4', 'Rel_02_s4'],\n",
    "                ['Rel_02_s4', 'Em_03_s4'],\n",
    "                ['WM_09_s4', 'Gamb_01_s4'],\n",
    "                ['WM_09_s4', 'Lang_03_s4', 'Soc_01_s4'],\n",
    "                ['WM_09_s4', 'Lang_03_s4', 'Em_03_s4'],\n",
    "                ['WM_09_s4', 'Lang_03_s4', 'Rel_02_s4'],\n",
    "                ['Rel_02_s4', 'Em_03_s4', 'Lang_03_s4'],\n",
    "                ['WM_09_s4', 'Lang_03_s4', 'Em_03_s4', 'Gamb_01_s4'],\n",
    "                ['WM_09_s4', 'Lang_03_s4', 'Em_03_s4', 'Soc_01_s4'],\n",
    "                ['WM_09_s4', 'Lang_03_s4', 'Em_03_s4', 'Soc_01_s4', 'Gamb_01_s4', 'Rel_02_s4'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using single map data\n",
    "for score in ['g_efa_cv', 'PMAT24_A_CR', 'ReadEng_Unadj', 'NEOFAC_O']:\n",
    "    single_scan_results=[]\n",
    "    for con in task_contrasts:\n",
    "        print(con)\n",
    "        for dtype in ['orig', 'pred']:\n",
    "            print(dtype)\n",
    "            data = read_features(f'data/{con}_z_masked_{dtype}.csv', subjlist=subjlist)\n",
    "            dfs, behav = bbs.match_dfs_by_ind([data], hcp_df)\n",
    "            bbs_model = bbs.BBSPredictSingle(data=dfs[0], target=behav[score], num_components=75,\n",
    "                                      folds=10, model=glm, groups=behav['Family_group'])\n",
    "            bbs_model.predict()\n",
    "            bbs_model.build_contribution_map()\n",
    "            p_val = bbs_model.permutation_test(5000)\n",
    "            r_mean, r_std = bbs_model.stats['r'].mean(), bbs_model.stats['r'].std()\n",
    "            mse_mean, mse_std = bbs_model.stats['mse'].mean(), bbs_model.stats['mse'].std()\n",
    "            single_scan_results.append([con, dtype, r_mean, r_std, mse_mean, mse_std, p_val]) \n",
    "            print(bbs_model.summary)\n",
    "            print(p_val)\n",
    "            bbs.to_pickle(bbs_model, f'bbs_models/{con}_{dtype}_{score}_10folds.pickle')\n",
    "            save_masked_maps(bbs_model.contribution_map, f'consensus_maps/{con}_{dtype}_{score}_10folds')\n",
    "            np.savetxt(f'predicted_values/{con}_{dtype}_{score}.csv',bbs_model.predicted, delimiter=',')\n",
    "    single_scan_df = pd.DataFrame(data = single_scan_results,\n",
    "                columns = ['input','type','r_mean','r_std','mse_mean','mse_std', 'p'])\n",
    "    single_scan_df.to_csv(f'{score}_single_scan_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using multi-map data\n",
    "\n",
    "for score in ['g_efa_cv', 'PMAT24_A_CR', 'ReadEng_Unadj', 'NEOFAC_O']:\n",
    "    multi_scan_results=[]\n",
    "    for comb in combinations:\n",
    "        final_num_comps=300\n",
    "        num_comps=int(final_num_comps/len(comb))\n",
    "        ff_num =160\n",
    "        input_name = '+'.join(comb)\n",
    "        print(input_name)\n",
    "        for dtype in ['orig', 'pred']:\n",
    "            print(dtype)\n",
    "            dfs = [read_features(f'data/{con}_z_masked_{dtype}.csv', subjlist=subjlist) for con in comb]\n",
    "            dfs, behav = bbs.match_dfs_by_ind(dfs, hcp_df)\n",
    "            bbs_model = bbs.BBSpredictMulti(data=dfs, target=behav[score], num_components=num_comps, \n",
    "                                            final_feature_number=ff_num, folds=10, model=elnet, groups=behav['Family_group'])\n",
    "            bbs_model.predict()\n",
    "            bbs_model.build_contribution_map()\n",
    "            p_val = bbs_model.permutation_test(5000)\n",
    "            r_mean, r_std = bbs_model.stats['r'].mean(), bbs_model.stats['r'].std()\n",
    "            mse_mean, mse_std = bbs_model.stats['mse'].mean(), bbs_model.stats['mse'].std()\n",
    "            multi_scan_results.append([input_name, dtype, r_mean, r_std, mse_mean, mse_std, p_val]) \n",
    "            print(bbs_model.summary)\n",
    "            bbs.to_pickle(bbs_model, f'bbs_models/{input_name}_{dtype}_{score}_10folds.pickle')\n",
    "            save_masked_maps(bbs_model.contribution_map, f'consensus_maps/{input_name}_{dtype}_{score}_10folds')\n",
    "            np.savetxt(f'predicted_values/{input_name}_{dtype}_{score}.csv',bbs_model.predicted, delimiter=',')\n",
    "    multi_scan_df = pd.DataFrame(data = multi_scan_results,\n",
    "                columns = ['input','type','r_mean','r_std','mse_mean','mse_std', 'p'])\n",
    "    multi_scan_df.to_csv(f'{score}_multi_scan_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict using connectome and alff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in ['g_efa_cv','PMAT24_A_CR', 'ReadEng_Unadj', 'NEOFAC_O']:\n",
    "    print(score)\n",
    "    rs_results = []\n",
    "    print('alff')\n",
    "    data = read_features(f'data/alff.csv', subjlist=subjlist)\n",
    "    dfs, behav = bbs.match_dfs_by_ind([data], hcp_df)\n",
    "    bbs_model = bbs.BBSPredictSingle(data=dfs[0], target=behav[score], num_components=75,\n",
    "                              folds=10, model=glm, groups=behav['Family_group'])\n",
    "    bbs_model.predict()\n",
    "    print(bbs_model.summary)\n",
    "    p_val = bbs_model.permutation_test(5000)\n",
    "    r_mean, r_std = bbs_model.stats['r'].mean(), bbs_model.stats['r'].std()\n",
    "    mse_mean, mse_std = bbs_model.stats['mse'].mean(), bbs_model.stats['mse'].std()\n",
    "    rs_results.append(['alff', '-', r_mean, r_std, mse_mean, mse_std, pval]) \n",
    "\n",
    "    print('connectome')\n",
    "    data = read_features(f'data/rs_conn.csv', subjlist=subjlist)\n",
    "    dfs, behav = bbs.match_dfs_by_ind([data], hcp_df)\n",
    "    bbs_model = bbs.BBSPredictSingle(data=dfs[0], target=behav[score], num_components=75,\n",
    "                              folds=10, model=glm, groups=behav['Family_group'])\n",
    "    bbs_model.predict()\n",
    "    print(bbs_model.summary)\n",
    "    p_val = bbs_model.permutation_test(5000)\n",
    "    r_mean, r_std = bbs_model.stats['r'].mean(), bbs_model.stats['r'].std()\n",
    "    mse_mean, mse_std = bbs_model.stats['mse'].mean(), bbs_model.stats['mse'].std()\n",
    "    np.savetxt(f'predicted_values/rs_conn_{score}.csv',bbs_model.predicted, delimiter=',')\n",
    "\n",
    "    rs_results.append(['rs_conn', '-', r_mean, r_std, mse_mean, mse_std, p_val]) \n",
    "\n",
    "    rs_results = pd.DataFrame(data = rs_results,\n",
    "                 columns = ['input','type','r_mean','r_std','mse_mean','mse_std', 'p'])\n",
    "    rs_results.to_csv(f'{score}_rs_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare all predictions to connectome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_conn = read_features(f'data/rs_conn.csv', subjlist=subjlist)\n",
    "single_data_compare_scores = []\n",
    "for con in ['WM_09_s4', 'Soc_01_s4', 'Gamb_01_s4', 'Em_03_s4', 'Rel_02_s4', 'Lang_03_s4']:\n",
    "    print(con)\n",
    "    for dtype in ['orig', 'pred']:\n",
    "        print(dtype)\n",
    "        data = read_features(f'data/{con}_z_masked_{dtype}.csv', subjlist=subjlist)\n",
    "        s, p, _ = bbs_comparisons.compare_predictions_single(data, rs_conn, hcp_df, 'g_efa_cv', k=1000, train_size=0.9)\n",
    "        single_data_compare_scores.append([con, dtype, s, p])\n",
    "        print(p)\n",
    "        \n",
    "single_data_compare_scores_df = pd.DataFrame(single_data_compare_scores, columns=['input', 'type', 's', 'p'])\n",
    "single_data_compare_scores_df.to_csv('single_scan_comprison_stats_family_1000_g_efa_cv_10split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_conn = read_features(f'data/rs_conn.csv', subjlist=subjlist)\n",
    "\n",
    "multi_data_compare_scores = []\n",
    "for comb in combinations:\n",
    "    input_name = '+'.join(comb)\n",
    "    print(input_name)\n",
    "    for dtype in ['orig', 'pred']:\n",
    "        data = [read_features(f'data/{con}_z_masked_{dtype}.csv', subjlist=subjlist) for con in comb]\n",
    "        s,p = bbs_comparisons.compare_predictions_multi(data, rs_conn, hcp_df, 'g_efa_cv', 0.9, k=1000)\n",
    "        print(p)\n",
    "        multi_data_compare_scores.append([input_name, dtype, s, p])\n",
    "\n",
    "\n",
    "multi_data_compare_scores_df = pd.DataFrame(multi_data_compare_scores, columns=['input', 'type', 's', 'p'])\n",
    "multi_data_compare_scores_df.to_csv('multi_scan_comprison_stats_family_1000_g_efa_cv_10split.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare prediction ratios and diffs (in-out of scanner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm_2bk_orig_z = read_features(f'data/WM_09_s4_z_masked_orig.csv', subjlist=subjlist)\n",
    "wm_2bk_pred_z = read_features(f'data/WM_09_s4_z_masked_pred.csv', subjlist=subjlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comapre_ratios_results = []\n",
    "comapre_diffs_results = []\n",
    "for score in ['g_efa_cv', 'ReadEng_Unadj', 'PMAT24_A_CR']: \n",
    "    ratios, diffs = bbs_comparisons.compare_prediction_ratios(wm_2bk_orig_z,wm_2bk_pred_z,hcp_df,hcp_df_wm,score,'WM_Task_2bk_Acc')\n",
    "    comapre_ratios_results.append([score,ratios['s'],ratios['p']])\n",
    "    comapre_diffs_results.append([score,diffs['s'],diffs['p']])\n",
    "comapre_ratios_results_df = pd.DataFrame(comapre_ratios_results, columns=['score', 'statistic', 'p'])\n",
    "comapre_ratios_results_df.to_csv('comapre_ratios_results_1000iter_family.csv')\n",
    "comapre_diffs_results_df = pd.DataFrame(comapre_diffs_results, columns=['score', 'statistic', 'p'])\n",
    "comapre_diffs_results_df.to_csv('comapre_diffs_results_1000iter_family.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = ['g_efa_cv', 'ReadEng_Unadj', 'PMAT24_A_CR', 'WM_Task_2bk_Acc']\n",
    "inputs = {'wm_orig': 'WM_09_s4_z_masked_orig.csv', 'wm_pred': 'WM_09_s4_z_masked_pred.csv', 'rs_conn': 'rs_conn.csv'}\n",
    "results = []\n",
    "for d in inputs.keys():\n",
    "    print(d)\n",
    "    for score in scores:\n",
    "        print(score)\n",
    "        data = read_features(f'data/{inputs[d]}', subjlist=subjlist)\n",
    "        if score == 'WM_Task_2bk_Acc':\n",
    "            dfs, behav = bbs.match_dfs_by_ind([data], hcp_df_wm)\n",
    "        else:       \n",
    "            dfs, behav = bbs.match_dfs_by_ind([data], hcp_df)\n",
    "        bbs_model = bbs.BBSPredictSingle(data=dfs[0], target=behav[score], num_components=75,\n",
    "                                  folds=10, model=glm, groups=behav['Family_group'])\n",
    "        bbs_model.predict()\n",
    "        results.append([d, score, np.mean(bbs_model.stats['r']), np.std(bbs_model.stats['r']), np.mean(bbs_model.stats['mse']), np.std(bbs_model.stats['mse'])])\n",
    "wm_results_df = pd.DataFrame(results, columns=['input', 'score', 'r_mean', 'r_std', 'mse_mean', 'mse_std'])\n",
    "wm_results_df.to_csv('wm_prediction_results_g_wm2bk_1.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
